{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3001bbf1-0724-40d6-9e6b-a8c538b23ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54895b0-97de-428d-8c96-7354b295107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main directory where model data is saved\n",
    "mainDataDirectory = \"C:/Users/seyed/Music/Brain Tumor - Model Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5aa6a7-dea8-4192-b3be-61461f677ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories of image classification\n",
    "imageCategories = [\"Yes\", \"No\"]\n",
    "\n",
    "# Directories for training and testing data\n",
    "trainDirectory = os.path.join(mainDataDirectory, 'Train')\n",
    "testDirectory = os.path.join(mainDataDirectory, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c50070-915b-4be8-a111-9ea3205bc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read images from a directory and add labels to each image\n",
    "def read_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for category in imageCategories:\n",
    "        category_path = os.path.join(directory, category)\n",
    "        label = imageCategories.index(category)\n",
    "        \n",
    "        for filename in os.listdir(category_path):\n",
    "            image_path = os.path.join(category_path, filename)\n",
    "            image = cv2.imread(image_path)\n",
    "            images.append(image)\n",
    "            if label == 0:\n",
    "                labels.append(\"Yes\")\n",
    "            elif label == 1:\n",
    "                labels.append(\"No\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66804a8c-f853-46b4-add6-86e529311aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training images\n",
    "train_x, train_y = read_images_from_directory(trainDirectory)\n",
    "\n",
    "# Read testing images\n",
    "test_x, test_y = read_images_from_directory(testDirectory)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc59ff8-2cc3-4cb1-87d5-83e558e50a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41104, 256, 256, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the number of data\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01ab0d3-f883-49fe-a8ff-aad2d8c74930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10276, 256, 256, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cfb6eb-ae13-42d5-bbda-461192d61ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes'], dtype='<U3')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the categories in the dataset\n",
    "np.unique(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1473cee1-1862-4f13-a0d7-e61a0a2bf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map string labels to integer labels inorder for the model to read the data in integer format\n",
    "label_mapping = {'No': 1, 'Yes': 0}\n",
    "\n",
    "# Convert string labels to integer labels\n",
    "train_y = [label_mapping[label] for label in train_y]\n",
    "test_y = [label_mapping[label] for label in test_y]\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "train_y_one_hot = to_categorical(train_y, num_classes=2)\n",
    "test_y_one_hot = to_categorical(test_y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513931a5-a623-4604-9467-64ba8698cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2569/2569 [==============================] - 2268s 883ms/step - loss: 0.4637 - accuracy: 0.9429 - val_loss: 0.0392 - val_accuracy: 0.9877\n",
      "Epoch 2/10\n",
      "2569/2569 [==============================] - 2265s 882ms/step - loss: 0.1314 - accuracy: 0.9553 - val_loss: 0.0428 - val_accuracy: 0.9880\n",
      "Epoch 3/10\n",
      "2569/2569 [==============================] - 2265s 882ms/step - loss: 0.1192 - accuracy: 0.9622 - val_loss: 0.0246 - val_accuracy: 0.9919\n",
      "Epoch 4/10\n",
      "2569/2569 [==============================] - 2260s 880ms/step - loss: 0.1124 - accuracy: 0.9665 - val_loss: 0.0470 - val_accuracy: 0.9887\n",
      "Epoch 5/10\n",
      "2569/2569 [==============================] - 2267s 882ms/step - loss: 0.0959 - accuracy: 0.9712 - val_loss: 0.0796 - val_accuracy: 0.9848\n",
      "Epoch 6/10\n",
      "2569/2569 [==============================] - 2271s 884ms/step - loss: 0.0880 - accuracy: 0.9736 - val_loss: 0.0187 - val_accuracy: 0.9946\n",
      "Epoch 7/10\n",
      "2569/2569 [==============================] - 2261s 880ms/step - loss: 0.1003 - accuracy: 0.9710 - val_loss: 0.0413 - val_accuracy: 0.9911\n",
      "Epoch 8/10\n",
      "2569/2569 [==============================] - 2247s 875ms/step - loss: 0.0770 - accuracy: 0.9782 - val_loss: 0.0172 - val_accuracy: 0.9955\n",
      "Epoch 9/10\n",
      "2569/2569 [==============================] - 2257s 879ms/step - loss: 0.0726 - accuracy: 0.9796 - val_loss: 0.0290 - val_accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "2569/2569 [==============================] - 2275s 885ms/step - loss: 0.0810 - accuracy: 0.9792 - val_loss: 0.0256 - val_accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "# Running the tf in cpu\n",
    "with tf.device('/cpu:0'):\n",
    "    # Loading the vgg16 model and setting up the image size to train\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    \n",
    "    # Freeze the layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build the model\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    # Train the model\n",
    "    history = model.fit(train_x, train_y_one_hot, epochs=10, batch_size=16, validation_data=(test_x, test_y_one_hot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfe3efb-90c3-4782-b410-e07baf7252b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('C:/Users/seyed/Music/Brain Tumor - Model/TumorDetectionModel_VGG-16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed73cec6-79f5-41f8-a6af-8159af89dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               8388864   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,104,066\n",
      "Trainable params: 8,389,378\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ca3c8-f902-44f2-ae06-c399ff05b4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
